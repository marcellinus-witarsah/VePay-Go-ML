{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df248e05",
   "metadata": {},
   "source": [
    "# License Plate Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b3950",
   "metadata": {},
   "source": [
    "## Installation for Using NVIDIA GPU Device\n",
    "License Plate Object Detection is the part of our Deep Learning Pipeline where we need to identify Region of Interest of the license plate that we want to recognize. Our Object Detection model will be using a ***YOLOv5*** method which has been created by ***ultralytics***. All Credits goes to every people who are involve in bringing YOLOv5 Method to live. The code can be accessed using this link https://github.com/ultralytics/yolov5.\n",
    "\n",
    "The training of the data will be using NVIDIA GeForce GTX 1660 Ti device. But there are some things that we need to prepare for this project such as:\n",
    "1. Installing CUDA Toolkit version 10.2 (use this [link](https://developer.nvidia.com/cuda-10.2-download-archive) for downloading it)\n",
    "2. Installing CuDNN version 8.3.0 (or pick other version that is compatible with CUDA Toolkit version, check this [link](https://developer.nvidia.com/rdp/cudnn-archive) for further information)\n",
    "3. Installing NVIDIA driver from this [link](https://www.nvidia.com/download/index.aspx) and choose the driver based on your GPU device name and type.\n",
    "4. Installing Visual Studio 2019 using this [link](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=community&rel=16&utm_medium=microsoft&utm_source=docs.microsoft.com&utm_campaign=download+from+relnotes&utm_content=vs2019ga+button) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebacca",
   "metadata": {},
   "source": [
    "## Setting Up Environment\n",
    "Python environment can be created using [anaconda](https://www.anaconda.com/) or [pipenv](https://pipenv.pypa.io/en/latest/) package by Python. In this project, pipenv is a tool that has been chosen for setting up environment. For starting things off, download the any Python version and then run ***pip install pipenv*** for installing pipenv package. Then setting up the environment at your project directory folder by running ***python -m pipenv --python 3.8.6***. Then you want to access or activate the environment using ***python -m pipenv shell***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6d18",
   "metadata": {},
   "source": [
    "Next, we need to install libaries to enable pytorch to access GPU by installing python packages by using this command\n",
    "***pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html***\n",
    "\n",
    "This command can be run in the jupyter notebook or in the command line (***make sure to run the command in the python environment we just created***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb187ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.9.0+cu102 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (1.9.0+cu102)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu102 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.10.0+cu102)\n",
      "Requirement already satisfied: torchaudio==0.9.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torch==1.9.0+cu102) (4.2.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torchvision==0.10.0+cu102) (9.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torchvision==0.10.0+cu102) (1.22.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# # run this command if it takes too long just run it on the command prompt, inside the python environment\n",
    "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22eadcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.9.0+cu102 (NVIDIA GeForce GTX 1660 Ti)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63317b",
   "metadata": {},
   "source": [
    "## Clone Repository\n",
    "Clone yolov5 repository by ***ultralytics*** from this link https://github.com/ultralytics/yolov5\n",
    "\n",
    "***Make sure that git has already installed in your computer and enabled to be executed from any file directory*** (set git into the environment variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61b16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating dcf8073..6dd6aea\n",
      "Fast-forward\n",
      " data/Argoverse.yaml    | 2 +-\n",
      " data/VOC.yaml          | 8 ++++----\n",
      " export.py              | 4 ++--\n",
      " models/experimental.py | 4 ++--\n",
      " models/yolo.py         | 6 +++---\n",
      " 5 files changed, 12 insertions(+), 12 deletions(-)\n",
      "yolov5 already exists and up to date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/ultralytics/yolov5\n",
      "   dcf8073..6dd6aea  master          -> origin/master\n",
      "   e50fd93..b2c7124  ultralytics/HUB -> origin/ultralytics/HUB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 5)) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 6)) (1.22.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 7)) (4.5.5.64)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 8)) (9.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 10)) (2.27.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 12)) (1.9.0+cu102)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 13)) (0.10.0+cu102)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 14)) (4.64.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 15)) (3.19.4)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 18)) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 22)) (1.4.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 23)) (0.11.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 35)) (8.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 36)) (5.9.1)\n",
      "Requirement already satisfied: thop in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 37)) (0.0.31.post2005241907)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 10)) (1.26.6)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torch>=1.7.0->-r requirements.txt (line 12)) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tqdm>=4.41.0->-r requirements.txt (line 14)) (0.4.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (2.6.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.37.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (62.3.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.46.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 22)) (2022.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (3.0.29)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (5.2.1.post0)\n",
      "Requirement already satisfied: decorator in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from ipython->-r requirements.txt (line 35)) (2.12.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (5.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 35)) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (4.11.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 35)) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from stack-data->ipython->-r requirements.txt (line 35)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from stack-data->ipython->-r requirements.txt (line 35)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from stack-data->ipython->-r requirements.txt (line 35)) (0.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 18)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 18)) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (3.5.2)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.22.4)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (0.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.27.1)\n",
      "Requirement already satisfied: certifi==2021.5.30 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3==1.26.6 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.26.6)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (4.64.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (0.20.0)\n",
      "Requirement already satisfied: idna==2.10 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: wget in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (3.2)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (6.0)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (4.5.5.64)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib->roboflow) (20.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib->roboflow) (4.33.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests->roboflow) (2.0.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# clone the repo from this link 'https://github.com/ultralytics/yolov5.git'\n",
    "git_https = 'https://github.com/ultralytics/yolov5.git'\n",
    "folder_name = 'yolov5'\n",
    "if not os.path.exists(os.path.join(os.getcwd(), folder_name)):\n",
    "    !git clone {git_https}\n",
    "else:\n",
    "    !cd yolov5 && git pull\n",
    "    print(folder_name, 'already exists and up to date')\n",
    "\n",
    "# install requirements of yolov5\n",
    "!cd yolov5 && pip install -r requirements.txt\n",
    "# install roboflow for data pulling\n",
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438aad67",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "After collecting our data, we will be using roboflow (can be accessed from this [link](https://roboflow.com/)) which is a tool for anotating region of interest. In this case, the region of interest is license plate. After anotating, we can export into any form of a dataset that will be accepted by our model. We can export it manually or using an API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a46e687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in vepay-license-plate-detection-1 to yolov5pytorch: 100% [288856323 / 288856323] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to vepay-license-plate-detection-1 in yolov5pytorch:: 100%|â–ˆ| 6666/6666 [00:03<00:00, 18\n"
     ]
    }
   ],
   "source": [
    "# pulling costum-data created from roboflow website using API\n",
    "####################################TEMPLATE EXAMPLE#########################################\n",
    "from roboflow import Roboflow\n",
    "# # Version 1\n",
    "# rf = Roboflow(api_key=\"fdqIcHgbgdGJz86WHnwU\")\n",
    "# project = rf.workspace(\"licenseplatedetection-p7ijb\").project(\"vepay-license-plate-detection\")\n",
    "# dataset = project.version(1).download(\"yolov5\")\n",
    "# Version 2\n",
    "# rf = Roboflow(api_key=\"fdqIcHgbgdGJz86WHnwU\")\n",
    "# project = rf.workspace(\"licenseplatedetection-p7ijb\").project(\"vepay-license-plate-detection\")\n",
    "# dataset = project.version(2).download(\"yolov5\")\n",
    "os.chdir('datasets')\n",
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"fdqIcHgbgdGJz86WHnwU\")\n",
    "# project = rf.workspace(\"licenseplatedetection-p7ijb\").project(\"vepay-license-plate-detection\")\n",
    "# dataset = project.version(18).download(\"yolov5\")\n",
    "rf = Roboflow(api_key=\"QO4DMMa7NTdZ7ZLqskzh\")\n",
    "project = rf.workspace(\"capstone-project-wpxpu\").project(\"vepay-license-plate-detection-wufwj\")\n",
    "dataset = project.version(1).download(\"yolov5\")\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af661e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset already exists\n",
      "location of dataset =  C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\n"
     ]
    }
   ],
   "source": [
    "# Setting up location for the dataset\n",
    "DATASET_PARENT_FOLDER = os.path.join(os.getcwd(), 'datasets')\n",
    "DATASET_FOLDER_NAME = 'vepay-license-plate-detection-1' # filled later becase the dataset is still being collected\n",
    "DATASET_LOCATION = os.path.join(DATASET_PARENT_FOLDER, DATASET_FOLDER_NAME)\n",
    "\n",
    "if not os.path.exists(DATASET_LOCATION):\n",
    "    print(\"dataset doesn't exists\")\n",
    "else:\n",
    "    print(\"dataset already exists\")\n",
    "    \n",
    "\n",
    "print('location of dataset = ', DATASET_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943beedf",
   "metadata": {},
   "source": [
    "Checking up the data.yaml inside the **DATASET_LOCATION** because the object detection API that is used needs it to find information where we put our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8212ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names = ['license-plate']\n",
      "nc = 1\n",
      "train = ../datasets/vepay-license-plate-detection-1/train/images\n",
      "val = ../datasets/vepay-license-plate-detection-1/valid/images\n",
      "num of classes =  1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# take a look inside the data.yaml file\n",
    "import yaml\n",
    "with open(os.path.join(DATASET_LOCATION, \"data.yaml\"), \"r\") as stream:\n",
    "    try:\n",
    "        content = yaml.safe_load(stream) \n",
    "        # print the content of data.yaml file\n",
    "        # we need to change the train and val path\n",
    "        for key, vals in content.items():\n",
    "            print(key, '=', vals)\n",
    "            \n",
    "        num_classes= content['nc']\n",
    "        print('num of classes = ', num_classes)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# number of classes\n",
    "print(num_classes)\n",
    "\n",
    "# make sure that the path can be seen by the yolov5 folder where it is the directory for running the python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590f930",
   "metadata": {},
   "source": [
    "## Configure Yolov5 Model\n",
    "We can do this configuring the .yaml file of that model that has been provided inside the yolov5 repo that we clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d9222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################## Yolo V5 default version ##############################\n",
      "custom_yolov5l.yaml\n",
      "custom_yolov5l6.yaml\n",
      "custom_yolov5m.yaml\n",
      "custom_yolov5m6.yaml\n",
      "custom_yolov5n.yaml\n",
      "custom_yolov5n6.yaml\n",
      "custom_yolov5s.yaml\n",
      "custom_yolov5s6.yaml\n",
      "custom_yolov5x.yaml\n",
      "custom_yolov5x6.yaml\n",
      "yolov5l.yaml\n",
      "yolov5m.yaml\n",
      "yolov5n.yaml\n",
      "yolov5s.yaml\n",
      "yolov5x.yaml\n",
      "\n",
      "############################## Yolo V5 version 6 ##############################\n",
      "yolov5l6.yaml\n",
      "yolov5m6.yaml\n",
      "yolov5n6.yaml\n",
      "yolov5s6.yaml\n",
      "yolov5x6.yaml\n",
      "\n",
      "############################## Yolo V5 version 6 ##############################\n",
      "yolov5l.yaml\n",
      "yolov5m.yaml\n",
      "yolov5n.yaml\n",
      "yolov5s.yaml\n",
      "yolov5x.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_directory = os.path.join(os.getcwd(), 'yolov5', 'models')\n",
    "print('\\n' + 30*'#'+ ' Yolo V5 default version ' + 30*'#')\n",
    "for file in os.listdir(model_directory):\n",
    "    if file.endswith('.yaml'):\n",
    "        print(file)\n",
    "\n",
    "# copy yolo5 version 6 .yaml file\n",
    "print('\\n' + 30*'#'+ ' Yolo V5 version 6 ' + 30*'#')\n",
    "for file in os.listdir(model_directory+'/hub'):\n",
    "    if file in ['yolov5l6.yaml',  'yolov5m6.yaml', 'yolov5n6.yaml', 'yolov5s6.yaml', 'yolov5x6.yaml']:\n",
    "        print(file)\n",
    "\n",
    "# copy yolov5 .yaml file\n",
    "print('\\n' + 30*'#'+ ' Yolo V5 version 6 ' + 30*'#')\n",
    "for file in os.listdir(model_directory):\n",
    "    if file in ['yolov5l.yaml',  'yolov5m.yaml', 'yolov5n.yaml', 'yolov5s.yaml', 'yolov5x.yaml']:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011496af",
   "metadata": {},
   "source": [
    "For this project we will be using the Yolo V5 version 6 as our pretrained model for transfer learning the size of the model will be picked based capability of local machine. Don't use heavy model for training if teh hardware capabilty can't keep up with it. There must be a trade off between using **-small size and accuracy model-** or **-big size and high accuracy model-**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac8575c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov5l6.yaml\n",
      "{'nc': 80, 'depth_multiple': 1.0, 'width_multiple': 1.0, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5m6.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.67, 'width_multiple': 0.75, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5n6.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5s6.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5x6.yaml\n",
      "{'nc': 80, 'depth_multiple': 1.33, 'width_multiple': 1.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "yolov5_v6_yamls = [\n",
    "    'yolov5l6.yaml',  \n",
    "    'yolov5m6.yaml', \n",
    "    'yolov5n6.yaml', \n",
    "    'yolov5s6.yaml', \n",
    "    'yolov5x6.yaml',\n",
    "]\n",
    "\n",
    "for file in os.listdir(os.path.join(model_directory, 'hub')):\n",
    "    if file in yolov5_v6_yamls:\n",
    "        with open(os.path.join(model_directory, 'hub', file), \"r\") as stream:\n",
    "            try:\n",
    "                content = yaml.safe_load(stream)\n",
    "                print(file)\n",
    "                print(content, '\\n')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3ed2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov5l.yaml\n",
      "{'nc': 80, 'depth_multiple': 1.0, 'width_multiple': 1.0, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5m.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.67, 'width_multiple': 0.75, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5n.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.25, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5s.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5x.yaml\n",
      "{'nc': 80, 'depth_multiple': 1.33, 'width_multiple': 1.25, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "yolov5_yamls = [\n",
    "    'yolov5l.yaml',  \n",
    "    'yolov5m.yaml', \n",
    "    'yolov5n.yaml', \n",
    "    'yolov5s.yaml', \n",
    "    'yolov5x.yaml',\n",
    "]\n",
    "\n",
    "for file in os.listdir(os.path.join(model_directory)):\n",
    "    if file in yolov5_yamls:\n",
    "        with open(os.path.join(model_directory, file), \"r\") as stream:\n",
    "            try:\n",
    "                content = yaml.safe_load(stream)\n",
    "                print(file)\n",
    "                print(content, '\\n')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1226308",
   "metadata": {},
   "source": [
    "We can just change the ***nc*** into any number according to number of class that we want to predict. In this case, we use ***1*** because we only want to predict one class only which is license plate. Create a file call custom model .yaml by copying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12d852d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_yolov5l6.yaml already exists\n",
      "custom_yolov5m6.yaml already exists\n",
      "custom_yolov5n6.yaml already exists\n",
      "custom_yolov5s6.yaml already exists\n",
      "custom_yolov5x6.yaml already exists\n"
     ]
    }
   ],
   "source": [
    "# create a new costume .yaml folder to enable the model to predict 1 class only\n",
    "# copy the contents of the .yaml file which is identified by the model's name\n",
    "model_folder = os.path.join(os.getcwd(), 'yolov5', 'models', 'hub')\n",
    "\n",
    "for file in os.listdir(model_directory+'/hub'):\n",
    "    if file in yolov5_v6_yamls:\n",
    "        yaml_ori_path = os.path.join(model_directory, 'hub', file)\n",
    "        custom_yaml_name = 'custom_'+file\n",
    "        custom_yaml_path = os.path.join(model_directory, custom_yaml_name)\n",
    "        \n",
    "        if not os.path.exists(custom_yaml_path):\n",
    "            shutil.copy(yaml_ori_path, custom_yaml_path)\n",
    "            print(yaml_ori_path, 'has been copied to ', custom_yaml_path)\n",
    "        else:\n",
    "            print(custom_yaml_name, 'already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc67555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_yolov5l.yaml already exists\n",
      "custom_yolov5m.yaml already exists\n",
      "custom_yolov5n.yaml already exists\n",
      "custom_yolov5s.yaml already exists\n",
      "custom_yolov5x.yaml already exists\n"
     ]
    }
   ],
   "source": [
    "# create a new costume .yaml folder to enable the model to predict 1 class only\n",
    "# copy the contents of the .yaml file which is identified by the model's name\n",
    "model_folder = os.path.join(os.getcwd(), 'yolov5', 'models', 'hub')\n",
    "\n",
    "for file in os.listdir(model_directory):\n",
    "    if file in yolov5_yamls:\n",
    "        yaml_ori_path = os.path.join(model_directory, file)\n",
    "        custom_yaml_name = 'custom_'+file\n",
    "        custom_yaml_path = os.path.join(model_directory, custom_yaml_name)\n",
    "        \n",
    "        if not os.path.exists(custom_yaml_path):\n",
    "            shutil.copy(yaml_ori_path, custom_yaml_path)\n",
    "            print(yaml_ori_path, 'has been copied to ', custom_yaml_path)\n",
    "        else:\n",
    "            print(custom_yaml_name, 'already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99710519",
   "metadata": {},
   "source": [
    "Edit the .yaml file manually by using a notepad or using other code editor. The result can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6699d7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_yolov5l6.yaml\n",
      "{'nc': 1, 'depth_multiple': 1.0, 'width_multiple': 1.0, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5m6.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.67, 'width_multiple': 0.75, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5n6.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.33, 'width_multiple': 0.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5s6.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5x6.yaml\n",
      "{'nc': 1, 'depth_multiple': 1.33, 'width_multiple': 1.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results after editing the .yaml files for model configuration\n",
    "custom_yolov5_v6_yamls = [\n",
    "    'custom_yolov5l6.yaml',  \n",
    "    'custom_yolov5m6.yaml', \n",
    "    'custom_yolov5n6.yaml', \n",
    "    'custom_yolov5s6.yaml', \n",
    "    'custom_yolov5x6.yaml',\n",
    "]\n",
    "\n",
    "for file in os.listdir(model_directory):\n",
    "    if file in custom_yolov5_v6_yamls:\n",
    "        with open(os.path.join(model_directory, file), \"r\") as stream:\n",
    "            try:\n",
    "                content = yaml.safe_load(stream)\n",
    "                print(file)\n",
    "                print(content, '\\n')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d28311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_yolov5l.yaml\n",
      "{'nc': 1, 'depth_multiple': 1.0, 'width_multiple': 1.0, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5m.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.67, 'width_multiple': 0.75, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5n.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.33, 'width_multiple': 0.25, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5s.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5x.yaml\n",
      "{'nc': 1, 'depth_multiple': 1.33, 'width_multiple': 1.25, 'anchors': [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 14], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[17, 20, 23], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results after editing the .yaml files for model configuration\n",
    "custom_yolov5_v6_yamls = [\n",
    "    'custom_yolov5l.yaml',  \n",
    "    'custom_yolov5m.yaml', \n",
    "    'custom_yolov5n.yaml', \n",
    "    'custom_yolov5s.yaml', \n",
    "    'custom_yolov5x.yaml',\n",
    "]\n",
    "\n",
    "for file in os.listdir(model_directory):\n",
    "    if file in custom_yolov5_v6_yamls:\n",
    "        with open(os.path.join(model_directory, file), \"r\") as stream:\n",
    "            try:\n",
    "                content = yaml.safe_load(stream)\n",
    "                print(file)\n",
    "                print(content, '\\n')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35fc05",
   "metadata": {},
   "source": [
    "## Training Yolov5 Model\n",
    "Since we will be preparing a python script (from /yolov5/train.py) for training, there are some arguments that we need to consider which are:\n",
    "1. ***img:*** define input image size\n",
    "2. ***batch:*** determine batch size\n",
    "3. ***epochs:*** define the number of training epochs. (Note: often, 3000+ are common here!).\n",
    "4. ***data:*** set the path to our yaml file.\n",
    "5. ***cfg:*** specify our model configuration.\n",
    "6. ***weights:*** specify a custom path to weights (if the file doesn't exist the model pretrained weights will be downloaded automatically)\n",
    "7. ***cache:*** cache images for faster training. The default value for this argument is using 'RAM' we can change it to using 'disk' for more storage.\n",
    "8. ***project:*** folder for results\n",
    "9. ***name:*** result names inside the **project** folder\n",
    "10. ***freeze:*** for input how many layers that want to be freezed start from index zero to n-1 layer. For transfer learning we usually freeze the **backbone layers**. (transfer learning)\n",
    "11. ***hyp:*** hyperparameter for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd5e82",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668afba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_path = os.path.join(os.getcwd(), 'yolov5', 'data', 'hyps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81e919da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_yamls = ['hyp.scratch-high.yaml', 'hyp.scratch-med.yaml', 'hyp.scratch-low.yaml',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7258c667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-high.yaml\n",
      "custom_hyp.scratch-high.yaml already exists\n",
      "C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-low.yaml\n",
      "custom_hyp.scratch-low.yaml already exists\n",
      "C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-med.yaml\n",
      "custom_hyp.scratch-med.yaml already exists\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(hyperparameter_path):\n",
    "    if file in hyperparameter_yamls:\n",
    "        yaml_ori_path = os.path.join(hyperparameter_path, file)\n",
    "        custom_yaml_name = 'custom_'+file\n",
    "        custom_yaml_path = os.path.join(hyperparameter_path, custom_yaml_name)\n",
    "        print(custom_yaml_path)\n",
    "        \n",
    "        if not os.path.exists(custom_yaml_path):\n",
    "            shutil.copy(yaml_ori_path, custom_yaml_path)\n",
    "            print(yaml_ori_path, 'has been copied to ', custom_yaml_path)\n",
    "        else:\n",
    "            print(custom_yaml_name, 'already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933da1f6",
   "metadata": {},
   "source": [
    "Image augmentation is done to increase our variability as well the size of our dataset. **hyp.scratch-low.yaml** is the default path that is used by train.py script. We can change the path or just modify the file right away. These are the hyperparameters that will be edited. Hyperparameter that will be change are degrees, translate, shear, fliplr, and flipud. Doing the updates manually using code editor.\n",
    "\n",
    "***Default hyperparameters***\n",
    "\n",
    "```hsv_h: 0.015  # image HSV-Hue augmentation (fraction) \n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction) \n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction) \n",
    "degrees: 0.0  # image rotation (+/- deg) \n",
    "translate: 0.1  # image translation (+/- fraction) \n",
    "scale: 0.5  # image scale (+/- gain) \n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001 \n",
    "flipud: 0.0  # image flip up-down (probability) \n",
    "fliplr: 0.5  # image flip left-right (probability) \n",
    "mosaic: 1.0  # image mosaic (probability) \n",
    "mixup: 0.0  # image mixup (probability)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4afb91",
   "metadata": {},
   "source": [
    "### Training Script Generation \n",
    "Using Yolov5-P5 for transfer Learning because the models is suitable for 640x640 pixels. while Yolov5-P6 models include an extra P6/64 output layer for detection of larger objects, and benefit the most from training at higher resolution, like 1280x1280 pixels. I already tried trained on using 1280 as image size, it crashed. For this reason, Yolov5-P5 models will be used to train a 640 image size (source: https://zenodo.org/record/4679653#.Yo3ph6hBxPZ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22840057",
   "metadata": {},
   "source": [
    "#### Transfer Learning (Yolov5S)\n",
    "Freezing Backbone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f9869b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the commmand on the command line\n",
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 25 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5s.yaml --weights yolov5s.pt --workers 0 --cache disk --project lpod_train_results --name train_vlpd1_5s_freeze_backbone_ --freeze 10\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 32\n",
    "epochs = 25\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5s.yaml')\n",
    "weights = 'yolov5s.pt'\n",
    "workers= 0\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_vlpd1_5s_freeze_backbone_'\n",
    "freeze = 10\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {}\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze)\n",
    "print(\"Run the commmand on the command line\")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cec9fa",
   "metadata": {},
   "source": [
    "#### Transfer Learning (Yolov5S)\n",
    "Freezing Backbone and adding hyperparameter tuning (low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e26d66e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the commmand on the command line\n",
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 25 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5s.yaml --weights yolov5s.pt --workers 0 --cache disk --project lpod_train_results --name train_vlpd1_5s_freeze_backbone_hyp_low --freeze 10 --hyp C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-low.yaml\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 32\n",
    "epochs = 25\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5s.yaml')\n",
    "weights = 'yolov5s.pt'\n",
    "workers= 0\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_vlpd1_5s_freeze_backbone_hyp_low'\n",
    "freeze = 10\n",
    "hyp = os.path.join(os.getcwd(), 'yolov5', 'data', 'hyps', 'custom_hyp.scratch-low.yaml')\n",
    "\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {} --hyp {}\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze, hyp)\n",
    "print(\"Run the commmand on the command line\")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af48403",
   "metadata": {},
   "source": [
    "#### Transfer Learning (Yolov5S)\n",
    "Freezing Backbone and adding hyperparameter tuning (med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b182d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the commmand on the command line\n",
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 25 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5s.yaml --weights yolov5s.pt --workers 0 --cache disk --project lpod_train_results --name train_vlpd1_5s_freeze_backbone_hyp_med --freeze 10 --hyp C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-med.yaml\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 32\n",
    "epochs = 25\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5s.yaml')\n",
    "weights = 'yolov5s.pt'\n",
    "workers= 0\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_vlpd1_5s_freeze_backbone_hyp_med'\n",
    "freeze = 10\n",
    "hyp = os.path.join(os.getcwd(), 'yolov5', 'data', 'hyps', 'custom_hyp.scratch-med.yaml')\n",
    "\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {} --hyp {}\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze, hyp)\n",
    "print(\"Run the commmand on the command line\")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a49c4b",
   "metadata": {},
   "source": [
    "#### Transfer Learning (Yolov5S)\n",
    "Freezing Backbone and adding hyperparameter tuning (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5463297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the commmand on the command line\n",
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 25 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5s.yaml --weights yolov5s.pt --workers 0 --cache disk --project lpod_train_results --name train_vlpd1_5s_freeze_backbone_hyp_high --freeze 10 --hyp C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-high.yaml\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 32\n",
    "epochs = 25\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5s.yaml')\n",
    "weights = 'yolov5s.pt'\n",
    "workers= 0\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_vlpd1_5s_freeze_backbone_hyp_high'\n",
    "freeze = 10\n",
    "hyp = os.path.join(os.getcwd(), 'yolov5', 'data', 'hyps', 'custom_hyp.scratch-high.yaml')\n",
    "\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {} --hyp {}\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze, hyp)\n",
    "print(\"Run the commmand on the command line\")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd0c55",
   "metadata": {},
   "source": [
    "### Transfer Learning (Yolov5M)\n",
    "Freezing Backbone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ccc735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the commmand on the command line\n",
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 25 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5m.yaml --weights yolov5m.pt --workers 0 --cache disk --project lpod_train_results --name train_vlpd18_5m_freeze_backbone_ --freeze 15\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 32\n",
    "epochs = 25\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5m.yaml')\n",
    "weights = 'yolov5m.pt'\n",
    "workers= 0\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_vlpd18_5m_freeze_backbone_'\n",
    "freeze = 15\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {}\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze)\n",
    "print(\"Run the commmand on the command line\")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6df7a",
   "metadata": {},
   "source": [
    "#### Transfer Learning (Yolov5M)\n",
    "Freezing Backbone and adding hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce976f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the commmand on the command line\n",
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 25 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5m.yaml --weights yolov5m.pt --workers 0 --cache disk --project lpod_train_results --name train_vlpd18_5m_freeze_backbone_hyp_low --freeze 10 --hyp C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\hyps\\custom_hyp.scratch-low.yaml\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 32\n",
    "epochs = 25\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5m.yaml')\n",
    "weights = 'yolov5m.pt'\n",
    "workers= 0\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_vlpd18_5m_freeze_backbone_hyp_low'\n",
    "freeze = 10\n",
    "hyp = os.path.join(os.getcwd(), 'yolov5', 'data', 'hyps', 'custom_hyp.scratch-low.yaml')\n",
    "\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {} --hyp {}\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze,hyp)\n",
    "print(\"Run the commmand on the command line\")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fd730",
   "metadata": {},
   "source": [
    "## Evaluating Model\n",
    "This step is of evaluation we will check its precision, recall, etc. Using tools from weight and biases it will be a lot easier, because they provide visualization at a whole different level. Confidence threshold can be configured and we can see it realtime in the dashboard after training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e1440f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure that the wandb package has already installed\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887e7f1",
   "metadata": {},
   "source": [
    "Run the above command from the command prompt (**inside the python environment that has been created**) so that we can see the training logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcabec2",
   "metadata": {},
   "source": [
    "### metrics/mAP_0.5\n",
    "<img src=\"images/yolov5s-mAP-0.5-metrics-reports.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a9696",
   "metadata": {},
   "source": [
    "### metrics/mAP_0.5:0.95\n",
    "<img src=\"images/yolov5s-mAP-0.5_0.95-metrics-reports.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c73ad",
   "metadata": {},
   "source": [
    "### metrics/precision\n",
    "<img src=\"images/yolov5s-precision-metrics-reports.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077bcd9",
   "metadata": {},
   "source": [
    "### metrics/recall\n",
    "<img src=\"images/yolov5s-recall-metrics-reports.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bb8b1",
   "metadata": {},
   "source": [
    "### Table Metrics Comparison Between Models\n",
    "The models are compared based on the best version of themselves.\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Model Name</th>\n",
    "    <th>mAP_0.5</th>\n",
    "    <th>mAP_0.5:0.95</th>\n",
    "    <th>Precision</th>\n",
    "    <th>Recall</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_vlpd1_5s_freeze_backbone_</td>\n",
    "    <td>0.9949</td>\n",
    "    <td>0.7</td>\n",
    "    <td>0.9943</td>\n",
    "    <td>0.9876</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_vlpd1_5s_freeze_backbone_hyp_low</td>\n",
    "    <td>0.9946</td>\n",
    "    <td>0.6955</td>\n",
    "    <td>0.9954</td>\n",
    "    <td>0.9876</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_vlpd1_5s_freeze_backbone_hyp_med</td>\n",
    "    <td>0.995</td>\n",
    "    <td>0.6985</td>\n",
    "    <td>0.9985</td>\n",
    "    <td>0.9969</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_vlpd1_5s_freeze_backbone_hyp_high</td>\n",
    "    <td>0.995</td>\n",
    "    <td>0.6996</td>\n",
    "    <td>0.9905</td>\n",
    "    <td>0.9969</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764da7fe",
   "metadata": {},
   "source": [
    "## Validating Yolov5 Model\n",
    "The reason we need to validate the model is just to be sure that our model perform as expected. The difference between validation that is being run by this script and the training script are lies within the argument of Intersection Over Union (IOU) threshold and confidence threshold. These two arguments determine how our model will perform by selecting object abouve the ***confidence threshold*** and selecting bounding box which has an IOU above ***IOU threshold***. there are some arguments that we need to consider which are:\n",
    "1. ***img:*** define input image size\n",
    "2. ***data:*** set the path to our yaml file.\n",
    "3. ***weights:*** specify a custom path from our trained model weights\n",
    "4. ***conf-thres:*** threshold to determine which object to choose that has confidence value above it\n",
    "5. ***iou-thres:*** threshold to determine which bounding box to choose that has IOU value above it\n",
    "6. ***verbose:*** report mean average precision by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd72c57",
   "metadata": {},
   "source": [
    "<img src=\"images/iou.jpeg\">\n",
    "\n",
    "*source: https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410ac6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd yolov5 && python val.py --img 640 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-15\\data.yaml --weights C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/train_results/train6/weights/best.pt --conf-thres 0.5 --iou-thres 0.5 --verbose\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "weights = 'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/train_results/train6/weights/best.pt'\n",
    "conf_thres = 0.5\n",
    "iou_thres = 0.5\n",
    "\n",
    "command = \"cd yolov5 && python val.py --img {} --data {} --weights {} --conf-thres {} --iou-thres {} --verbose\".\\\n",
    "          format(img, data, weights, conf_thres, iou_thres)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b181142",
   "metadata": {},
   "source": [
    "## Inferencing using Yolov5 Model\n",
    "Since we will be preparing a python script (from /yolov5/detect.py) for testing, there are some arguments that we need to consider which are:\n",
    "1. ***img:*** define input image size\n",
    "2. ***source:*** input data\n",
    "3. ***conf:*** confidence level treshold\n",
    "4. ***weights:*** specify a custom path from our trained model weights\n",
    "5. ***name:*** file name of the detect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "155900fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd yolov5 && python detect.py --weights C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/lpod_train_results/train_vlpd1_5s_freeze_backbone_/weights/best.pt C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/lpod_train_results/train_vlpd1_5s_freeze_backbone_hyp_low/weights/best.pt C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/lpod_train_results/train_vlpd1_5s_freeze_backbone_hyp_med/weights/best.pt  --img 640 --conf 0.6 --source C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/datasets/vepay-license-plate-detection-1/test/images\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "img=640\n",
    "source= 'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/datasets/vepay-license-plate-detection-1/test/images'\n",
    "conf =0.6\n",
    "arr_weights = ['C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/lpod_train_results/train_vlpd1_5s_freeze_backbone_/weights/best.pt',\n",
    "          'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/lpod_train_results/train_vlpd1_5s_freeze_backbone_hyp_low/weights/best.pt',\n",
    "          'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/lpod_train_results/train_vlpd1_5s_freeze_backbone_hyp_med/weights/best.pt',]\n",
    "\n",
    "weights=''\n",
    "\n",
    "for weight in arr_weights:\n",
    "    weights += (weight+' ')\n",
    "\n",
    "command = \"cd yolov5 && python detect.py --weights {} --img {} --conf {} --source {}\".\\\n",
    "          format(weights, img, conf, source)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae51da8",
   "metadata": {},
   "source": [
    "## Export Model to TensorflowJS\n",
    "Lastly, we export the model for deployment in Website using Javascript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "878eb3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# export model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model_weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPlease input model path = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd yolov5 && python export.py --weights \u001b[39m\u001b[38;5;132;01m{model_weights_path}\u001b[39;00m\u001b[38;5;124m --include tfjs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.virtualenvs\\license-plate-object-detection-uHB0SpUB\\lib\\site-packages\\ipykernel\\kernelbase.py:1161\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[1;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\license-plate-object-detection-uHB0SpUB\\lib\\site-packages\\ipykernel\\kernelbase.py:1205\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# export model\n",
    "model_weights_path = input('Please input model path = ')\n",
    "!cd yolov5 && python export.py --weights {model_weights_path} --include tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(os.getcwd(), 'yolov5', 'train_results', 'train9', 'weights', 'best_saved_model')\n",
    "# export_path = os.path.join(os.getcwd(), 'yolov5', 'export_tfjs', 'best_web_model')\n",
    "# !tensorflowjs_converter \\\n",
    "#     --input_format=tf_saved_model \\\n",
    "#     --output_format=tfjs_graph_model\\\n",
    "#     --signature_name=serving_default\\\n",
    "#     --saved_model_tags=serve \\\n",
    "#     {model_path} \\\n",
    "#     {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bd66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "license-plate-object-detection (pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
