{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df248e05",
   "metadata": {},
   "source": [
    "# License Plate Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b3950",
   "metadata": {},
   "source": [
    "## Installation for Using NVIDIA GPU Device\n",
    "License Plate Object Detection is the part of our Deep Learning Pipeline where we need to identify Region of Interest of the license plate that we want to recognize. Our Object Detection model will be using a ***YOLOv5*** method which has been created by ***ultralytics***. All Credits goes to every people who are involve in bringing YOLOv5 Method to live. The code can be accessed using this link https://github.com/ultralytics/yolov5.\n",
    "\n",
    "The training of the data will be using NVIDIA GeForce GTX 1660 Ti device. But there are some things that we need to prepare for this project such as:\n",
    "1. Installing CUDA Toolkit version 10.2 (use this [link](https://developer.nvidia.com/cuda-10.2-download-archive) for downloading it)\n",
    "2. Installing CuDNN version 8.3.0 (or pick other version that is compatible with CUDA Toolkit version, check this [link](https://developer.nvidia.com/rdp/cudnn-archive) for further information)\n",
    "3. Installing NVIDIA driver from this [link](https://www.nvidia.com/download/index.aspx) and choose the driver based on your GPU device name and type.\n",
    "4. Installing Visual Studio 2019 using this [link](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=community&rel=16&utm_medium=microsoft&utm_source=docs.microsoft.com&utm_campaign=download+from+relnotes&utm_content=vs2019ga+button) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebacca",
   "metadata": {},
   "source": [
    "## Setting Up Environment\n",
    "Python environment can be created using [anaconda](https://www.anaconda.com/) or [pipenv](https://pipenv.pypa.io/en/latest/) package by Python. In this project, pipenv is a tool that has been chosen for setting up environment. For starting things off, download the any Python version and then run ***pip install pipenv*** for installing pipenv package. Then setting up the environment at your project directory folder by running ***python -m pipenv --python 3.8.6***. Then you want to access or activate the environment using ***python -m pipenv shell***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b6d18",
   "metadata": {},
   "source": [
    "Next, we need to install libaries to enable pytorch to access GPU by installing python packages by using this command\n",
    "***pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html***\n",
    "\n",
    "This command can be run in the jupyter notebook or in the command line (***make sure to run the command in the python environment we just created***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb187ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.9.0+cu102 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (1.9.0+cu102)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu102 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.10.0+cu102)\n",
      "Requirement already satisfied: torchaudio==0.9.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torch==1.9.0+cu102) (4.2.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torchvision==0.10.0+cu102) (9.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torchvision==0.10.0+cu102) (1.22.3)\n"
     ]
    }
   ],
   "source": [
    "# # run this command if it takes too long just run it on the command prompt, inside the python environment\n",
    "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22eadcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\.virtualenvs\\license-plate-object-detection-uHB0SpUB\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.9.0+cu102 (NVIDIA GeForce GTX 1660 Ti)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63317b",
   "metadata": {},
   "source": [
    "## Clone Repository\n",
    "Clone yolov5 repository by ***ultralytics*** from this link https://github.com/ultralytics/yolov5\n",
    "\n",
    "***Make sure that git has already installed in your computer and enabled to be executed from any file directory*** (set git into the environment variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61b16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 9a7f289..f43cd53\n",
      "Fast-forward\n",
      " .github/workflows/docker.yml | 20 ++++++++++----------\n",
      " export.py                    | 24 ++++++++++++------------\n",
      " utils/torch_utils.py         |  6 +++---\n",
      " 3 files changed, 25 insertions(+), 25 deletions(-)\n",
      "yolov5 already exists and up to date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From https://github.com/ultralytics/yolov5\n",
      "   9a7f289..f43cd53  master                 -> origin/master\n",
      " * [new branch]      apple/mps              -> origin/apple/mps\n",
      " * [new branch]      update/ConvTranspose2d -> origin/update/ConvTranspose2d\n",
      "   2bfd221..98ccebc  v7.0                   -> origin/v7.0\n",
      "   cabe64f..6e7532c  v7.0-dwconv2dtranspose -> origin/v7.0-dwconv2dtranspose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 4)) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 5)) (1.22.3)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 6)) (4.5.5.64)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 7)) (9.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 8)) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 9)) (2.27.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 11)) (1.9.0+cu102)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 12)) (0.10.0+cu102)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 13)) (4.64.0)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 16)) (2.9.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 20)) (1.4.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
      "Requirement already satisfied: thop in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from -r requirements.txt (line 37)) (0.0.31.post2005241907)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.5.30)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tqdm>=4.41.0->-r requirements.txt (line 13)) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.46.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (62.3.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.20.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (2.6.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2022.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (5.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.22.3)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (0.9.1)\n",
      "Requirement already satisfied: idna==2.10 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (0.20.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (6.0)\n",
      "Requirement already satisfied: certifi==2021.5.30 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2021.5.30)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (4.64.0)\n",
      "Requirement already satisfied: wget in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (3.2)\n",
      "Requirement already satisfied: urllib3==1.26.6 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.26.6)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.4.7)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.3.1)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (9.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from roboflow) (3.5.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib->roboflow) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from matplotlib->roboflow) (20.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests->roboflow) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "# clone the repo from this link 'https://github.com/ultralytics/yolov5.git'\n",
    "git_https = 'https://github.com/ultralytics/yolov5.git'\n",
    "folder_name = 'yolov5'\n",
    "if not os.path.exists(os.path.join(os.getcwd(), folder_name)):\n",
    "    !git clone {git_https}\n",
    "else:\n",
    "    !cd yolov5 && git pull\n",
    "    print(folder_name, 'already exists and up to date')\n",
    "\n",
    "# install requirements of yolov5\n",
    "!cd yolov5 && pip install -r requirements.txt\n",
    "# install roboflow for data pulling\n",
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438aad67",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "After collecting our data, we will be using roboflow (can be accessed from this [link](https://roboflow.com/)) which is a tool for anotating region of interest. In this case, the region of interest is license plate. After anotating, we can export into any form of a dataset that will be accepted by our model. We can export it manually or using an API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46e687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in vepay-license-plate-detection-1 to yolov5pytorch: 100% [58313195 / 58313195] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to vepay-license-plate-detection-1 in yolov5pytorch:: 100%|█| 2500/2500 [00:01<00:00, 18\n"
     ]
    }
   ],
   "source": [
    "# pulling costum-data created from roboflow website using API\n",
    "####################################TEMPLATE EXAMPLE#########################################\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"fdqIcHgbgdGJz86WHnwU\")\n",
    "project = rf.workspace(\"licenseplatedetection-p7ijb\").project(\"vepay-license-plate-detection\")\n",
    "dataset = project.version(1).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af661e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset already exists\n",
      "location of dataset =  C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\n"
     ]
    }
   ],
   "source": [
    "# Setting up location for the dataset\n",
    "DATASET_PARENT_FOLDER = os.path.join(os.getcwd(), 'datasets')\n",
    "DATASET_FOLDER_NAME = 'vepay-license-plate-detection-1' # filled later becase the dataset is still being collected\n",
    "DATASET_LOCATION = os.path.join(DATASET_PARENT_FOLDER, DATASET_FOLDER_NAME)\n",
    "\n",
    "if not os.path.exists(DATASET_LOCATION):\n",
    "    print(\"dataset doesn't exists\")\n",
    "else:\n",
    "    print(\"dataset already exists\")\n",
    "    \n",
    "\n",
    "print('location of dataset = ', DATASET_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943beedf",
   "metadata": {},
   "source": [
    "Checking up the data.yaml inside the **DATASET_LOCATION** because the object detection API that is used needs it to find information where we put our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8212ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names = ['license-plate']\n",
      "nc = 1\n",
      "train = ../datasets/vepay-license-plate-detection-1/train/images\n",
      "val = ../datasets/vepay-license-plate-detection-1/valid/images\n",
      "num of classes =  1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# take a look inside the data.yaml file\n",
    "import yaml\n",
    "with open(os.path.join(DATASET_LOCATION, \"data.yaml\"), \"r\") as stream:\n",
    "    try:\n",
    "        content = yaml.safe_load(stream) \n",
    "        # print the content of data.yaml file\n",
    "        # we need to change the train and val path\n",
    "        for key, vals in content.items():\n",
    "            print(key, '=', vals)\n",
    "            \n",
    "        num_classes= content['nc']\n",
    "        print('num of classes = ', num_classes)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# number of classes\n",
    "print(num_classes)\n",
    "\n",
    "# make sure that the path can be seen by the yolov5 folder where it is the directory for running the python script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590f930",
   "metadata": {},
   "source": [
    "## Configure Yolov5 Model\n",
    "We can do this configuring the .yaml file of that model that has been provided inside the yolov5 repo that we clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d9222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################## Yolo V5 default version ##############################\n",
      "custom_yolov5l6.yaml\n",
      "custom_yolov5m6.yaml\n",
      "custom_yolov5n6.yaml\n",
      "custom_yolov5s6.yaml\n",
      "custom_yolov5x6.yaml\n",
      "yolov5l.yaml\n",
      "yolov5m.yaml\n",
      "yolov5n.yaml\n",
      "yolov5s.yaml\n",
      "yolov5x.yaml\n",
      "\n",
      "############################## Yolo V5 version 6 ##############################\n",
      "yolov5l6.yaml\n",
      "yolov5m6.yaml\n",
      "yolov5n6.yaml\n",
      "yolov5s6.yaml\n",
      "yolov5x6.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_directory = os.path.join(os.getcwd(), 'yolov5', 'models')\n",
    "print('\\n' + 30*'#'+ ' Yolo V5 default version ' + 30*'#')\n",
    "for file in os.listdir(model_directory):\n",
    "    if file.endswith('.yaml'):\n",
    "        print(file)\n",
    "\n",
    "print('\\n' + 30*'#'+ ' Yolo V5 version 6 ' + 30*'#')\n",
    "for file in os.listdir(model_directory+'/hub'):\n",
    "    if file in ['yolov5l6.yaml',  'yolov5m6.yaml', 'yolov5n6.yaml', 'yolov5s6.yaml', 'yolov5x6.yaml']:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011496af",
   "metadata": {},
   "source": [
    "For this project we will be using the Yolo V5 version 6 as our pretrained model for transfer learning the size of the model will be picked based capability of local machine. Don't use heavy model for training if teh hardware capabilty can't keep up with it. There must be a trade off between using **-small size and accuracy model-** or **-big size and high accuracy model-**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac8575c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov5l6.yaml\n",
      "{'nc': 80, 'depth_multiple': 1.0, 'width_multiple': 1.0, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5m6.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.67, 'width_multiple': 0.75, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5n6.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5s6.yaml\n",
      "{'nc': 80, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "yolov5x6.yaml\n",
      "{'nc': 80, 'depth_multiple': 1.33, 'width_multiple': 1.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "yolov5_v6_yamls = [\n",
    "    'yolov5l6.yaml',  \n",
    "    'yolov5m6.yaml', \n",
    "    'yolov5n6.yaml', \n",
    "    'yolov5s6.yaml', \n",
    "    'yolov5x6.yaml',\n",
    "]\n",
    "\n",
    "for file in os.listdir(os.path.join(model_directory, 'hub')):\n",
    "    if file in yolov5_v6_yamls:\n",
    "        with open(os.path.join(model_directory, 'hub', file), \"r\") as stream:\n",
    "            try:\n",
    "                content = yaml.safe_load(stream)\n",
    "                print(file)\n",
    "                print(content, '\\n')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1226308",
   "metadata": {},
   "source": [
    "We can just change the ***nc*** into any number according to number of class that we want to predict. In this case, we use ***1*** because we only want to predict one class only which is license plate. Create a file call custom model .yaml by copying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12d852d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_yolov5l6.yaml already exists\n",
      "custom_yolov5m6.yaml already exists\n",
      "custom_yolov5n6.yaml already exists\n",
      "custom_yolov5s6.yaml already exists\n",
      "custom_yolov5x6.yaml already exists\n"
     ]
    }
   ],
   "source": [
    "# create a new costume .yaml folder to enable the model to predict 1 class only\n",
    "# copy the contents of the .yaml file which is identified by the model's name\n",
    "model_folder = os.path.join(os.getcwd(), 'yolov5', 'models', 'hub')\n",
    "\n",
    "for file in os.listdir(model_directory+'/hub'):\n",
    "    if file in yolov5_v6_yamls:\n",
    "        yaml_ori_path = os.path.join(model_directory, 'hub', file)\n",
    "        custom_yaml_name = 'custom_'+file\n",
    "        custom_yaml_path = os.path.join(model_directory, custom_yaml_name)\n",
    "        \n",
    "        if not os.path.exists(custom_yaml_path):\n",
    "            shutil.copy(yaml_ori_path, custom_yaml_path)\n",
    "            print(yaml_ori_path, 'has been copied to ', custom_yaml_path)\n",
    "        else:\n",
    "            print(custom_yaml_name, 'already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99710519",
   "metadata": {},
   "source": [
    "Edit the .yaml file manually by using a notepad or using other code editor. The result can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6699d7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_yolov5l6.yaml\n",
      "{'nc': 1, 'depth_multiple': 1.0, 'width_multiple': 1.0, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5m6.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.67, 'width_multiple': 0.75, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5n6.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.33, 'width_multiple': 0.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5s6.yaml\n",
      "{'nc': 1, 'depth_multiple': 0.33, 'width_multiple': 0.5, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n",
      "custom_yolov5x6.yaml\n",
      "{'nc': 1, 'depth_multiple': 1.33, 'width_multiple': 1.25, 'anchors': [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], 'backbone': [[-1, 1, 'Conv', [64, 6, 2, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 3, 'C3', [128]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 6, 'C3', [256]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 9, 'C3', [512]], [-1, 1, 'Conv', [768, 3, 2]], [-1, 3, 'C3', [768]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 3, 'C3', [1024]], [-1, 1, 'SPPF', [1024, 5]]], 'head': [[-1, 1, 'Conv', [768, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 8], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [512, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [256, 1, 1]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 3, 'C3', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 20], 1, 'Concat', [1]], [-1, 3, 'C3', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 16], 1, 'Concat', [1]], [-1, 3, 'C3', [768, False]], [-1, 1, 'Conv', [768, 3, 2]], [[-1, 12], 1, 'Concat', [1]], [-1, 3, 'C3', [1024, False]], [[23, 26, 29, 32], 1, 'Detect', ['nc', 'anchors']]]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results after editing the .yaml files for model configuration\n",
    "custom_yolov5_v6_yamls = [\n",
    "    'custom_yolov5l6.yaml',  \n",
    "    'custom_yolov5m6.yaml', \n",
    "    'custom_yolov5n6.yaml', \n",
    "    'custom_yolov5s6.yaml', \n",
    "    'custom_yolov5x6.yaml',\n",
    "]\n",
    "\n",
    "for file in os.listdir(model_directory):\n",
    "    if file in custom_yolov5_v6_yamls:\n",
    "        with open(os.path.join(model_directory, file), \"r\") as stream:\n",
    "            try:\n",
    "                content = yaml.safe_load(stream)\n",
    "                print(file)\n",
    "                print(content, '\\n')\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35fc05",
   "metadata": {},
   "source": [
    "## Training Yolov5 Model\n",
    "Since we will be preparing a python script (from /yolov5/train.py) for training, there are some arguments that we need to consider which are:\n",
    "1. ***img:*** define input image size\n",
    "2. ***batch:*** determine batch size\n",
    "3. ***epochs:*** define the number of training epochs. (Note: often, 3000+ are common here!).\n",
    "4. ***data:*** set the path to our yaml file.\n",
    "5. ***cfg:*** specify our model configuration.\n",
    "6. ***weights:*** specify a custom path to weights (if the file doesn't exist the model pretrained weights will be downloaded automatically)\n",
    "7. ***cache:*** cache images for faster training. The default value for this argument is using 'RAM' we can change it to using 'disk' for more storage.\n",
    "8. ***project:*** folder for results\n",
    "9. ***name:*** result names inside the **project** folder\n",
    "10. ***freeze:*** for input how many layers that want to be freezed start from index zero to n-1 layer. For transfer learning we usually freeze the **backbone layers**. (transfer learning)\n",
    "11. ***hyp:*** hyperparameter for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd5e82",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b38a41",
   "metadata": {},
   "source": [
    "Image augmentation is done to increase our variability as well the size of our dataset. **hyp.scratch-low.yaml** is the default path that is used by train.py script. We can change the path or just modify the file right away. These are the hyperparameters that will be edited. Hyperparameter that will be change are degrees, translate, shear, fliplr, and flipud.\n",
    "\n",
    "***Default hyperparameters***\n",
    "```hsv_h: 0.015  # image HSV-Hue augmentation (fraction) \n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction) \n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction) \n",
    "degrees: 0.0  # image rotation (+/- deg) \n",
    "translate: 0.1  # image translation (+/- fraction) \n",
    "scale: 0.5  # image scale (+/- gain) \n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001 \n",
    "flipud: 0.0  # image flip up-down (probability) \n",
    "fliplr: 0.5  # image flip left-right (probability) \n",
    "mosaic: 1.0  # image mosaic (probability) \n",
    "mixup: 0.0  # image mixup (probability)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f20506",
   "metadata": {},
   "source": [
    "***Edited hyperparameters***\n",
    "``` hsv_h: 0.015  # image HSV-Hue augmentation (fraction) \n",
    " hsv_s: 0.7  # image HSV-Saturation augmentation (fraction) \n",
    " hsv_v: 0.4  # image HSV-Value augmentation (fraction) \n",
    " degrees: 0.5  # image rotation (+/- deg) \n",
    " translate: 0.5  # image translation (+/- fraction) \n",
    " scale: 0.5  # image scale (+/- gain) \n",
    " shear: 0.5  # image shear (+/- deg) \n",
    " perspective: 0.0  # image perspective (+/- fraction), range 0-0.001 \n",
    " flipud: 1  # image flip up-down (probability) \n",
    " fliplr: 1  # image flip left-right (probability) \n",
    " mosaic: 1.0  # image mosaic (probability) \n",
    " mixup: 0.0  # image mixup (probability) ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4afb91",
   "metadata": {},
   "source": [
    "### Training Script Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "318ca579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd yolov5 && python train.py --img 640 --batch 32 --epochs 50 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\vepay-license-plate-detection-1\\data.yaml --cfg C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\models\\custom_yolov5s6.yaml --weights yolov5s6.pt --workers 3 --cache disk --project lpod_train_results --name train_ --freeze 0 1 2 3 4 5 6 7 8 9 10 11  --evolve\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "batch = 16\n",
    "epochs = 50\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "cfg = os.path.join(model_directory, 'custom_yolov5s6.yaml')\n",
    "weights = 'yolov5s6.pt'\n",
    "workers= 3\n",
    "cache = 'disk'\n",
    "project = 'lpod_train_results' \n",
    "name = 'train_'\n",
    "freeze = ''\n",
    "for layer in range(12):\n",
    "    freeze += str(layer)+ ' '\n",
    "\n",
    "command = \"cd yolov5 && python train.py --img {} --batch {} --epochs {} --data {} --cfg {} --weights {} --workers {} --cache {} --project {} --name {} --freeze {} --evolve\".\\\n",
    "          format(img, batch, epochs, data, cfg, weights, workers, cache, project, name, freeze)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fd730",
   "metadata": {},
   "source": [
    "## Evaluating Model\n",
    "This step is of evaluation we will check its precision, recall, etc. Using tools from weight and biases it will be a lot easier, because they provide visualization at a whole different level. Confidence threshold can be configured and we can see it realtime in the dashboard after training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1440f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (0.12.16)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (1.5.12)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (62.3.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\.virtualenvs\\license-plate-object-detection-uhb0spub\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "# making sure that the wandb package has already installed\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887e7f1",
   "metadata": {},
   "source": [
    "Run the above command from the command prompt (**inside the python environment that has been created**) so that we can see the training logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764da7fe",
   "metadata": {},
   "source": [
    "## Validating Yolov5 Model\n",
    "The reason we need to validate the model is just to be sure that our model perform as expected. The difference between validation that is being run by this script and the training script are lies within the argument of Intersection Over Union (IOU) threshold and confidence threshold. These two arguments determine how our model will perform by selecting object abouve the ***confidence threshold*** and selecting bounding box which has an IOU above ***IOU threshold***. there are some arguments that we need to consider which are:\n",
    "1. ***img:*** define input image size\n",
    "2. ***data:*** set the path to our yaml file.\n",
    "3. ***weights:*** specify a custom path from our trained model weights\n",
    "4. ***conf-thres:*** threshold to determine which object to choose that has confidence value above it\n",
    "5. ***iou-thres:*** threshold to determine which bounding box to choose that has IOU value above it\n",
    "6. ***verbose:*** report mean average precision by class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd72c57",
   "metadata": {},
   "source": [
    "<img src=\"images/iou.jpeg\">\n",
    "\n",
    "*source: https://medium.com/analytics-vidhya/iou-intersection-over-union-705a39e7acef*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410ac6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd yolov5 && python val.py --img 640 --data C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\datasets\\Car-License-Plate-1\\data.yaml --weights C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/train_results/train6/weights/best.pt --conf-thres 0.5 --iou-thres 0.5 --verbose\n"
     ]
    }
   ],
   "source": [
    "img = 640\n",
    "data = os.path.join(DATASET_LOCATION, 'data.yaml')\n",
    "weights = 'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/train_results/train6/weights/best.pt'\n",
    "conf_thres = 0.5\n",
    "iou_thres = 0.5\n",
    "\n",
    "command = \"cd yolov5 && python val.py --img {} --data {} --weights {} --conf-thres {} --iou-thres {} --verbose\".\\\n",
    "          format(img, data, weights, conf_thres, iou_thres)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b181142",
   "metadata": {},
   "source": [
    "## Inferencing using Yolov5 Model\n",
    "Since we will be preparing a python script (from /yolov5/detect.py) for testing, there are some arguments that we need to consider which are:\n",
    "1. ***img:*** define input image size\n",
    "2. ***source:*** input data\n",
    "3. ***conf:*** confidence level treshold\n",
    "4. ***weights:*** specify a custom path from our trained model weights\n",
    "5. ***name:*** file name of the detect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "155900fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd yolov5 && python detect.py --weights C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/train_results/train6/weights/best.pt --img 640 --conf 0.5 --source C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/datasets/Car-License-Plate-1/test/images\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "img=640\n",
    "source= 'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/datasets/Car-License-Plate-1/test/images'\n",
    "conf =0.5\n",
    "weights = 'C:/Users/USER/Documents/GitHub/VePay-Go-ML/license-plate-object-detection/yolov5/train_results/train6/weights/best.pt'\n",
    "\n",
    "command = \"cd yolov5 && python detect.py --weights {} --img {} --conf {} --source {}\".\\\n",
    "          format(weights, img, conf, source)\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae51da8",
   "metadata": {},
   "source": [
    "## Export Model to TensorflowJS\n",
    "Lastly, we export the model for deployment in Website using Javascript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878eb3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input model path = C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best.pt\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(1, 640, 640, 3)]   0           []                               \n",
      "                                                                                                  \n",
      " tf_conv (TFConv)               (1, 320, 320, 32)    3488        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf_conv_1 (TFConv)             (1, 160, 160, 64)    18496       ['tf_conv[0][0]']                \n",
      "                                                                                                  \n",
      " tfc3 (TFC3)                    (1, 160, 160, 64)    18624       ['tf_conv_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_7 (TFConv)             (1, 80, 80, 128)     73856       ['tfc3[0][0]']                   \n",
      "                                                                                                  \n",
      " tfc3_1 (TFC3)                  (1, 80, 80, 128)     115200      ['tf_conv_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_15 (TFConv)            (1, 40, 40, 256)     295168      ['tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_2 (TFC3)                  (1, 40, 40, 256)     623872      ['tf_conv_15[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_25 (TFConv)            (1, 20, 20, 384)     885120      ['tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_3 (TFC3)                  (1, 20, 20, 384)     664704      ['tf_conv_25[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_31 (TFConv)            (1, 10, 10, 512)     1769984     ['tfc3_3[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_4 (TFC3)                  (1, 10, 10, 512)     1181184     ['tf_conv_31[0][0]']             \n",
      "                                                                                                  \n",
      " tfsppf (TFSPPF)                (1, 10, 10, 512)     656128      ['tfc3_4[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_conv_39 (TFConv)            (1, 10, 10, 384)     196992      ['tfsppf[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample (TFUpsample)       (1, 20, 20, 384)     0           ['tf_conv_39[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat (TFConcat)           (1, 20, 20, 768)     0           ['tf_upsample[0][0]',            \n",
      "                                                                  'tfc3_3[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_5 (TFC3)                  (1, 20, 20, 384)     812160      ['tf_concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_45 (TFConv)            (1, 20, 20, 256)     98560       ['tfc3_5[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample_1 (TFUpsample)     (1, 40, 40, 256)     0           ['tf_conv_45[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat_1 (TFConcat)         (1, 40, 40, 512)     0           ['tf_upsample_1[0][0]',          \n",
      "                                                                  'tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_6 (TFC3)                  (1, 40, 40, 256)     361216      ['tf_concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_51 (TFConv)            (1, 40, 40, 128)     32896       ['tfc3_6[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample_2 (TFUpsample)     (1, 80, 80, 128)     0           ['tf_conv_51[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat_2 (TFConcat)         (1, 80, 80, 256)     0           ['tf_upsample_2[0][0]',          \n",
      "                                                                  'tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_7 (TFC3)                  (1, 80, 80, 128)     90496       ['tf_concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_57 (TFConv)            (1, 40, 40, 128)     147584      ['tfc3_7[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_3 (TFConcat)         (1, 40, 40, 256)     0           ['tf_conv_57[0][0]',             \n",
      "                                                                  'tf_conv_51[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_8 (TFC3)                  (1, 40, 40, 256)     295680      ['tf_concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_63 (TFConv)            (1, 20, 20, 256)     590080      ['tfc3_8[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_4 (TFConcat)         (1, 20, 20, 512)     0           ['tf_conv_63[0][0]',             \n",
      "                                                                  'tf_conv_45[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_9 (TFC3)                  (1, 20, 20, 384)     713856      ['tf_concat_4[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_69 (TFConv)            (1, 10, 10, 384)     1327488     ['tfc3_9[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_5 (TFConcat)         (1, 10, 10, 768)     0           ['tf_conv_69[0][0]',             \n",
      "                                                                  'tf_conv_39[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_10 (TFC3)                 (1, 10, 10, 512)     1312256     ['tf_concat_5[0][0]']            \n",
      "                                                                                                  \n",
      " tf_detect (TFDetect)           ((1, 25500, 6),      23112       ['tfc3_7[0][0]',                 \n",
      "                                 [(1, 6400, 3, 6),                'tfc3_8[0][0]',                 \n",
      "                                 (1, 1600, 3, 6),                 'tfc3_9[0][0]',                 \n",
      "                                 (1, 400, 3, 6),                  'tfc3_10[0][0]']                \n",
      "                                 (1, 100, 3, 6)])                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,308,200\n",
      "Trainable params: 0\n",
      "Non-trainable params: 12,308,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\data\\coco128.yaml, weights=['C:\\\\Users\\\\USER\\\\Documents\\\\GitHub\\\\VePay-Go-ML\\\\license-plate-object-detection\\\\yolov5\\\\train_results\\\\train9\\\\weights\\\\best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['saved_model']\n",
      "YOLOv5  v6.1-201-g9a7f289 Python-3.8.6 torch-1.9.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "custom_YOLOv5s6 summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best.pt with output shape (1, 25500, 6) (24.0 MB)\n",
      "2022-05-19 15:57:12.123348: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-19 15:57:12.123381: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.9.0...\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "2022-05-19 15:57:15.138081: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-05-19 15:57:15.145340: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-CA2M8NU1\n",
      "2022-05-19 15:57:15.145500: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-CA2M8NU1\n",
      "2022-05-19 15:57:15.145900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1    885504  models.common.Conv                      [256, 384, 3, 2]              \n",
      "  8                -1  1    665856  models.common.C3                        [384, 384, 1]                 \n",
      "  9                -1  1   1770496  models.common.Conv                      [384, 512, 3, 2]              \n",
      " 10                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      " 11                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 12                -1  1    197376  models.common.Conv                      [512, 384, 1, 1]              \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  1    813312  models.common.C3                        [768, 384, 1, False]          \n",
      " 16                -1  1     98816  models.common.Conv                      [384, 256, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 20                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 24                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 27                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  1    715008  models.common.C3                        [512, 384, 1, False]          \n",
      " 30                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  1   1313792  models.common.C3                        [768, 512, 1, False]          \n",
      " 33  [23, 26, 29, 32]  1     23112  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [128, 256, 384, 512], [640, 640]]\n",
      "2022-05-19 15:57:22.148332: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-05-19 15:57:22.148568: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "Assets written to: C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best_saved_model\\assets\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success, saved as C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best_saved_model (47.3 MB)\n",
      "\n",
      "Export complete (15.34s)\n",
      "Results saved to \u001b[1mC:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\u001b[0m\n",
      "Detect:          python detect.py --weights C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best_saved_model\n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best_saved_model')\n",
      "Validate:        python val.py --weights C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\train_results\\train9\\weights\\best_saved_model\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "# export model\n",
    "model_weights_path = input('Please input model path = ')\n",
    "!cd yolov5 && python export.py --weights {model_weights_path} --include saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263aaa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing weight file C:\\Users\\USER\\Documents\\GitHub\\VePay-Go-ML\\license-plate-object-detection\\yolov5\\export_tfjs\\best_web_model\\model.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 16:26:49.205045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-19 16:26:49.205076: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-19 16:26:52.224897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-19 16:26:52.225749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2022-05-19 16:26:52.226578: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2022-05-19 16:26:52.232273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2022-05-19 16:26:52.233090: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2022-05-19 16:26:52.234620: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-19 16:26:52.234809: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:Importing a function (__inference_pruned_8902) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2022-05-19 16:26:52.845643: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2022-05-19 16:26:52.845846: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2022-05-19 16:26:52.847621: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(os.getcwd(), 'yolov5', 'train_results', 'train9', 'weights', 'best_saved_model')\n",
    "export_path = os.path.join(os.getcwd(), 'yolov5', 'export_tfjs', 'best_web_model')\n",
    "!tensorflowjs_converter \\\n",
    "    --input_format=tf_saved_model \\\n",
    "    --output_format=tfjs_graph_model\\\n",
    "    --signature_name=serving_default\\\n",
    "    --saved_model_tags=serve \\\n",
    "    {model_path} \\\n",
    "    {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407d854b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "license-plate-object-detection (pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
